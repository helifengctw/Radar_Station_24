# Radar_Station_24
Private radar station repository for CKYF Robomaster 2024

please contact with me if you have any problems about this project (emial: hlfctw@proton.me)


说明：

1，雷达硬件说明：雷达分为传感器端和运算端以及连接线材（这些部分都有尺寸限制，检录时会严格检查，具体细节参考“机器人制作规范手册”），其中传感器端包括三脚架、激光雷达、两个网口工业相机，运算端包括电脑、固定板、固定板上的裁判系统和电池，连接线材包括两个相机的网线和电源线、激光雷达的一个通信线和电源线（其中激光雷达和相机最终连到电脑的都是网线，电脑一般只有一个网口，所以还配置了一个USB网卡和雷电口网卡；电源线上连了稳压，只需连一个xt60口的电池就可以给整个传感器端供电；雷达还有另一套连接方案，相较与比赛用的那套多了两个网口POE交换机，既可以供电还可以传递信息，但缺点是需要插电，如果现在那一套不能用了，可以用带交换机的这一套，东西在雷达物资箱的袋子里）。雷达比赛启动程序：在进入赛场的候场区后可以线把线先全部连好，然后试一下能否正常启动。然后连好的线不要断开，把雷达传感器端放上雷达摆放平台，线材不要通过雷达传感器端摆放平台中间的那个孔，要从护栏上直接绕过来，这样摆上去就能用，不用再连线了，节约了很多时间，运算端放到下面，插好电脑电源，连裁判系统。打开终端，执行scripts/sensor_qidong脚本，然后再开一个终端，执行ros2 run utilities pick_up_point命令，进行pnp选点。选完点后要检查一下小地图上的点是否有明显的位置偏差，如果有则可能是选出的点的信息没有成功被读取，可迅速再选一次点。

2，radar24_ws中的包除了lidar_localization_ros2和ndt_omp_ros2完全没用到外（这几个包在25赛季的开发中可能用到，也可以看一下），其他的都用到了，必须根据scripts/sensor_qidong里面的东西一个一个看。

3，雷达站有左右两个相机和一个激光雷达，相机目前是一个看远一个看近，激光雷达是几乎覆盖全场。左右两个相机的运行代码几乎一样，所以将相机驱动、yolo识别、激光雷达驱动和获取深度包放在一个ros2组件中（component_sensor_far和component_sensor_close）,两个组件的代码相同，参数和命名空间不同，其中只有一个组件驱动激光雷达，另一个组件不驱动激光雷达只订阅重映射到该命名空间的点云话题。

4，采用组件的原因是提高内聚性和利用进程内通信，将图像话题和识别等放在一个进程内，但是我也没具体测试组件的优化作用到底大不大。

5，雷达的工作原理和算法可以参考战队仓库的22年雷达站的开源文档。

以下是各个包的具体说明：

a、bayer_camera_driver:采用bayer格式驱动相机获得图像，这个包的关键点在于相机曝光、增益等参数的调节，调不好的话对导致图像中运动物体的拖影长，画面太暗，帧率太低等问题。

b、yolov5_detector：主要涉及yolov5、KM_matching等算法。其中有两层yolov5神经网络，分别识别相机画面中的红方和蓝方的车，和第一次网络识别到的车中的装甲板数字。采用trt部署的yolov5，是从github上tensorrtx仓库改进过来的。KM_matching是用来进行帧之间的运动预测匹配，来增强识别的连续性。最后将识别出的框和车的id发送出来。

c、get_depth：订阅激光雷达的点云话题和yolo的识别结果话题。将点云投影的相机图像画面，并根据yolo的识别框按照一定策略取出对应车的深度。最后将车的框的深度和图像坐标发送出去。

d、small_map：订阅车的框的深度和图像坐标，并将其投影到小地图坐标系上，再加以判断，筛选出敌方车辆，将车在小地图上的坐标发送出去。并根据裁判系统的一些信息进行自主决策，判断出发双倍易伤的时机，再发送出去。

e、robot_serial：串口通信包，订阅小地图的处理结果，并与裁判系统通信，收发车辆的坐标和场地信息。

f、pnp_solver：进行pnp标定的计算，订阅utilites/pick_up_point的选点消息，进行pnp计算。同时，记录并更新各个参数，在small_map启动时，为其提供所需参数以进行计算。

g、utilites：工具包，目前只有pick_up_point一个节点，用于准备阶段进行pnp选点，并将点的信息发送出去。这个包具有两个模式，一个是既可以选图像中的点，也可以选三维座标系中点，用于适应性中的第一场比赛，不知道要选赛场三维座标系中的哪几个点；第二个模式是已知选三维座标系中要选的点，只需选出其在图像中对应的点即可。两个功能通过调整代码里的注释部分实现。

h、radar_interfaces：存放的雷达交互信息，包括一些消息服务以及参数等，其他功能包大部分都要依赖这个，具体要看CMakeLists和launch文件。


调试雷达需注意事项：
1，场地很重要，没有场地（篮球场大小，最好边上有个架子）就没办法准确的调试，也没办法找bug，所以一定要积极的找好场地，不能往后拖，越早找好场地越好。

2，相机和雷达之间的联合标定最为繁琐，也对雷达定位的准确度其决定性作用。每个相机和雷达之间联合标定所用的数据要达到40个以上，并且最终的计算误差至少要小于4.0，标定用的大标定板要有1.5m*1m以上才准确，你想要在多大的实际范围内保证一定的定位精度，就要在多大的范围内进行标定，标定板要在相机视野的各个角落都有一定量的数据样本。标定板要用支架固定，不能人举着。标定同一距离的标定板时可以不动标定板，通过调整雷达的角度来使标定板出现在相机视野的各个地方，就免得搬来搬去那么大个标定板。标定板也可以用几个标签作为四个角点贴在墙上，从而描出一个大小足够的矩形，通过调整雷达来测不同的距离，这样就免得买大标定板和搬了。

3，我的yolov5神经网络的数据集制作情况如下：第一层车的神经网络由于时间问题，只有1.3k张图像，效果还行，不会漏识别，但是重复识别和误识别的情况较多。第二层识别装甲板的神经网络数据集大概有1.5w张，识别效果还行，但是数字之间的误识别还是偶尔发生，可以考虑改进标图策略或者加一些传统视觉预处理。总之数据集的数量只能比我的多，同时要质量优先，没有质量，你再多的数据集都白搭。炼丹有很多经验小技巧，可以自己多练习练习，摸索一下。

4，雷达比赛的时候有很多注意事项，如果考虑不周，会直接影响成败。比如雷达站的机械结构，用三脚架还是其他支架支撑雷达，怎么连线、供电才能保证在比赛的准备阶段能够快速的启动雷达并保证较高的稳定和性。用三角架的话，比较轻便，好搬运，用一个方形支撑框的话，能够把框架卡在雷达摆放平台的一个角，这样就能保证雷达每次都在同一个位置。相机和雷达如果用插头供电就没办法去没电源的场地调，只有用电池供电才能在任何场景下调试。雷达之前一直是用自己的笔记本电脑跑的，如果有条件的话可以用台式机箱跑，算力更足，而且比赛会提供显示屏，但是缺点就是没办法在准备的时候提前启动程序，必须等摆到雷达摆放平台才能启动，会浪费大量时间。制作规范有尺寸要求，一切制作、设计都要按制作规范来。 

5，按照我看到的其他队伍情况，雷达站在新规则下的趋势是采用纯视觉方案，放弃使用激光雷达。优势包括一下几点：a、成本低。b、程序更简单，无须进行取深度等操作。c、如果用激光雷达就要进行联合标定，标定完就要保证雷达的相机的相对位置不会发生变化，否则要重新标定，很麻烦。如果只有相机则无须联合标定。d、纯视觉方案识别和定位的连续性更高，更准确。e、目前激光雷达方案效果还挺不错，研究也趋于饱和，要进行大范围的创新很难，如果尝试新方案，有老方案保底。缺点主要有：a、目前队里没有人搞过这个方案，所以一切从头开始，但是RM论坛有一些开源可以借鉴。b、具体方案不知，所以自己搞的话，能不能行得通也不确定。c、对识别的要求更高，需要采用一些预测和追踪算法来优化神经网络的结果。

6，可以在决策方面进行优化，联合自瞄和哨兵作出一套体系，进行更加全面的信息感知和更智能的决策。还可以加一些预警功能，如飞坡预警，英雄边上来敌方步兵的预警等等。

总结：以上所说，如果有任何不懂的地方，一定要弄懂，不能模糊不清，先自己搜索，实在弄不明白可以问我。这些注意事项全都是的惨痛教训得出经验，只能做的更好，不能降低要求。

